
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Narrating Long-form Videos with Multimodal In-Context Learning">
  <meta name="keywords" content="MM-ReAct, ChatGPT, GPT-4, GPT-4V">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XWFVEGQZPC"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-XWFVEGQZPC');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon-T.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
		/* Define the grid layout */
		.mygrid {
			display: grid;
			grid-template-columns: repeat(3, 1fr);
			grid-gap: 20px;
			width: 80%;
			margin: auto;
		}
		.grid_item {
      background: #FFFFFF;
      opacity: 1;
    }

		/* Define the size of the GIFs */
		.mygif {
			height: auto;
			cursor: pointer;
		}
		
		/* Define the modal styles */
		.modal {
			display: none;
			position: fixed;
			z-index: 1;
			left: 0;
			top: 0;
			width: 100%;
			height: 100%;
			overflow: auto;
			background-color: rgba(0,0,0,0.9);
		}
		
		.modal-content {
			margin: auto;
			display: block;
			width: 80%;
			max-width: 800px;
			max-height: 80%;
		}

    /* Define the full-screen overlay styles */
		.overlay {
			position: fixed;
			z-index: 999;
			left: 0;
			top: 0;
			width: 100%;
			height: 100%;
			overflow: hidden;
			background-color: rgba(0,0,0,0.9);
			display: none;
		}
		
		.overlay img {
			width: auto;
			height: 90%;
			margin: 0 auto;
			display: block;
			max-width: 90%;
			max-height: 90%;
		}

    /* Define the video styles */
		.gifvideo {
			width: 100%;
			height: auto;
		}

		/* Define the progress bar styles */
		.progress {
			width: 100%;
			height: 10px;
			background-color: #ddd;
			position: relative;
		}

		.progress-bar {
			height: 100%;
			background-color: #4CAF50;
			position: absolute;
			top: 0;
			left: 0;
		}
		
		/* Define the close button style */
		.close {
			color: white;
			position: absolute;
			top: 10px;
			right: 25px;
			font-size: 35px;
			font-weight: bold;
			cursor: pointer;
		}
		
		.close:hover,
		.close:focus {
			color: #bbb;
			text-decoration: none;
			cursor: pointer;
		}
	</style>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="./static/images/icon.png" alt="MM-Narrator" width="120"/></h1> 
          <h2 class="title is-2 publication-title" style="width: 110%; margin-left: -5%">MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning</h2>
          <div class="is-size-5">
            <span class="author-block">
              <a href="https://chaoyivision.github.io/" style="color:#00A4EF;font-weight:normal;" target="_blank">Chaoyi Zhang</a>
            </span>, 
            <span class="author-block">
              <a href="https://sites.google.com/site/kevinlin311tw/me" style="color:#00A4EF;font-weight:normal;" target="_blank">Kevin Lin</a>
            </span>, 
            <span class="author-block">
              <a href="https://zyang-ur.github.io/" style="color:#00A4EF;font-weight:normal;" target="_blank">Zhengyuan Yang</a>
            </span>, 
            <span class="author-block">
              <a href="http://jianfengwang.me/" style="color:#00A4EF;font-weight:normal;" target="_blank">Jianfeng Wang</a>
            </span>, 
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en" style="color:#00A4EF;font-weight:normal;" target="_blank">Linjie Li</a>
            </span>, 
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/chunglin/" style="color:#00A4EF;font-weight:normal;" target="_blank">Chung-Ching Lin</a>
            </span>, 
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/zliu/" style="color:#00A4EF;font-weight:normal;" target="_blank">Zicheng Liu</a>
            </span>, 
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/lijuanw/" style="color:#00A4EF;font-weight:normal;" target="_blank">Lijuan Wang</a>
            </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              University of Sydney&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Microsoft GenAI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Advanced Micro Devices</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://mm-narrator.github.io/preprint_MMNarrator.pdf" 
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Preprint (19MB)</span>
                </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a href="#data" target="_self" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://mm-narrator.github.io/" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <p class="subtitle has-text-centered">
        <b>MM-Narrator</b> is a training-free framework towards automatic <b>audio description (AD)</b> generation for <b>long-form videos</b> via iterations: for each scene, it perceives multimodal inputs (i.e., seeing <span style="color: rgb(162, 193, 207);font-weight: bold;">visual frames</span> and hearing <span style="color: rgb(71, 211, 90);font-weight: bold;">character dialogues</span>), recalls the <span style="color: rgb(216, 110, 204);font-weight: bold;">context AD</span> depicting past scenes, and infers <span style="color: rgb(233, 113, 50);font-weight: bold;">AD prediction</span> for the current scene.
      <p>
      <a href="images/full_teaser.png" target="popup"> <img id="teaser" width="100%" src="images/full_teaser.png"></a>


    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
We present MM-Narrator, a novel system leveraging GPT-4 with multimodal in-context learning for the generation of audio descriptions (AD). Unlike previous methods that primarily focused on downstream fine-tuning with short video clips, MM-Narrator excels in generating precise audio descriptions for videos of extensive lengths, even beyond hours, in an autoregressive manner. This capability is made possible by the proposed memory-augmented generation process, which effectively utilizes both the short-term textual context and long-term visual memory through an efficient register-and-recall mechanism. These contextual memories compile pertinent past information, including storylines and character identities, ensuring an accurate tracking and depicting of story-coherent and character-centric audio descriptions. Maintaining the training-free design of MM-Narrator, we further propose a complexity-based demonstration selection strategy to largely enhance its multi-step reasoning capability via few-shot multimodal in-context learning (MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator consistently outperforms both the existing fine-tuning-based approaches and LLM-based approaches in most scenarios, as measured by standard evaluation metrics. Additionally, we introduce the first segment-based evaluator for recurrent text generation. Empowered by GPT-4, this evaluator comprehensively reasons and marks AD generation performance in various extendable dimensions.          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <br>
    <br>
  


      <!-- ADGen. -->
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">AD Generation with MM-Narrator</h2>
            <div class="content has-text-justified">
              <a href="images/ADGen.png" target="popup"> <img id="ADGen" width="100%" src="images/ADGen.png"></a>
            </div>
          </div>
        </div>
        <!--/ ADGen. -->
        <br>
        <br>
    
      <!-- ADEval. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">AD Evaluation with <span style="font-family: 'Courier New';">SegEval</span></h2>
          <div class="content has-text-justified">
            <a href="images/ADEval.png" target="popup"> <img id="ADEval" width="100%" src="images/ADEval.png"></a>
          </div>
        </div>
      </div>
      <!--/ ADEval. -->
      <br>
      <br>

      <!-- Qual. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results on MAD-eval Benchmark</span></h2>
          <div class="content has-text-justified">
            <a href="images/full_qual.png" target="popup"> <img id="Qual" width="100%" src="images/full_qual.png"></a>
          </div>
        </div>
      </div>
      <!--/ Qual. -->
      <br>
      <br>


    <!-- Paper Model 2. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 id="data" class="title is-3">MM-Narrator Data</h2>
        <div class="content has-text-left">
          <p>
            We will release our data later, covering both AD generation and evaluation. For AD generation via MM-Narrator, we will release:
          <ul>
            <li>prompts and responses (i.e., generated ADs)</li>
            <li>intermediate outputs of multimodal experts</li>
            <li>CoTs articulated via LLM</li>
          </ul>
          For AD evaluation with <span style="font-family: 'Courier New';">SegEval</span>, we will release the prompts and responses (i.e., raw marks with reasoning).</li>
          </p>
        </div>        
       </h2>
        </div>
      </div>
    </div>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>
    <br>       



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2023mmnarrator,
  title= {MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning},
  author={Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, Lijuan Wang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
}
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      We are deeply grateful to <a href="https://openai.com">OpenAI</a> for providing access to  <a href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak">their exceptional tool</a>. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special acknowledgment to Faisal Ahmed, Ehsan Azarnasab, and Lin Liang for their constructive feedback. 
    </p>
    <br>
    <br>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


</body>
</html>
